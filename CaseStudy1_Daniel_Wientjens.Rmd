---
title: "Statistics in Action - case study 1"
author: "Toxicity assessment of the MON810 maize"
output:
  html_document:
    fig_height: 3
    fig_width: 8
    number_sections: yes
    toc: no
  pdf_document:
    fig_height: 3
    fig_width: 8
    includes:
      before_body: macros.tex
    keep_tex: yes
    number_sections: yes
    toc: no
---
These packages are required to run the code
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, ggplot2, tidyverse, nortest, pwr, equivalence)
```

</br>


#Introduction

The dataset <ttt>dataMON810_2018.csv</ttt> consists of several measurements made during a subchronic toxicity study concerning the MON810 maize.

Biochemical parameters reflecting most physiological functions were measured two times (week 5 and 14), in particular through serum and urine chemistry, and hematology. Organ weights were measured at week 14.

The main objective of this study is to evaluate possible GMO effects on these parameters.

##Single comparison
We consider the variable "CALCIUM".
First we load the data
```{r}
data.study <- read.csv('dataMON810_2018.csv')
```
   
**a. Test if the mean level of calcium for period 2 is the same for males and females.**  </br>**Hint:** plot first the data and justify the test(s) to use.
```{r}
#Splitting the dataset into the two periods 
data.study.p1 <- data.study %>% filter(period == 1)
data.study.p2 <- data.study %>% filter(period == 2)

#Splitting the dataset of period 2 into male and female and plotting the result
data.study.p2.m <- data.study.p2 %>% filter(sex == 'M')
data.study.p2.f <- data.study.p2 %>% filter(sex == 'F')

new <- aggregate(data.study.p2$CALCIUM ~ data.study.p2$sex, FUN= "mean") 
colnames(new) <- c('sex', 'calcium')

ggplot()+
  geom_point(data = data.study.p2.m, aes(x=sex, y=CALCIUM, colour = 'blue'), show.legend = F) + 
  geom_point(data = data.study.p2.f, aes(x=sex, y=CALCIUM, colour = 'red'), show.legend = F) +
  geom_point(data = new, aes(x=sex, y=calcium), size=5, alpha=0.5) + 
  ylab("Calcium Levels") +
  xlab("Sex")

boxplot(CALCIUM ~ sex, data = data.study.p2, col = "blue", ylab = "Calcium levels", xlab = "Sex")
```

```{r}
par(mfrow = c(1, 2))
hist(data.study.p2.f$CALCIUM, main='Histogram of calcium for females', xlab = "Calcium level for females")
hist(data.study.p2.m$CALCIUM, main='Histogram of calcium for male', xlab = "Calcium level for males")
```
```{r}
aggregate(data.study.p2$CALCIUM ~ data.study.p2$sex, FUN= "sd" )
aggregate(data.study.p2$CALCIUM ~ data.study.p2$sex, FUN= "mean" )
```
After all these plot we have to determine what kind of a distribution we have in front of us. If it is a normal distribution we should use a t-test otherwise we should use a Wilcoxon test.
So first we check whether the variances differ between the male and female polulation by performing an F-test. 
Only then we conduct a t-test since we need to know whether to leave the var.equal input at it's default FALSE value or to change it to TRUE. 
```{r}
alpha<-0.05

x <- data.study[data.study$sex=="M","CALCIUM"]
y <- data.study[data.study$sex=="F","CALCIUM"]
var.test(x,y, alternative = 'two.sided')

t.test(x,y, conf.level=1-alpha, var.equal = FALSE)
```
As we can see the F-test has pointed out that the variances are different. This is then used in the 
Since the p-value is very small, we can reject the H0 and say that the means of the control males and females is different.
However, we can actually check if the data is normaly distributed or not by running the Anderson-Darling test.
```{r}
ad.test(data.study.p2.f$CALCIUM)
ad.test(data.study.p2.m$CALCIUM)
```
The results show low p-values, lower than our significance level and hence we reject the (H0) assumption that the data is normaly distributed, so we decide to do a Wilcoxon test.
A Wilcoxon test since it is more powerfull due to lack of assumptions about the distribution of the data. 
It assigns numeric ranks to all the observations, beginning with 1 for the smallest value. Where there are groups of tied values, assign a rank equal to the midpoint of unadjusted rankings.
```{r}
m <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(CALCIUM) %>% na.omit()
f <- data.study %>% filter(sex == 'F', period == 2) %>% dplyr::select(CALCIUM) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
Also with this test we see that the p value is very small and hence the H0 can be rejected and that means that the means of the control males and females is different.
So both tests show that the means of calcium levels for males and females in period 2 are the different (since H0 is that they are the same/similar and due to the low p-value we reject H0).
In the end though, the Wilcoxon test is very much preffered.

**b. Test for the males if the mean level of calcium is the same for period 1 and period 2.**

Here we split the original dataset into the males for period 1 and period 2 and keep only the calcium column in order to do our t-test. But first we plot the boxplot and histogram to check the distribution out.
```{r}
x <- data.study %>% filter(sex == 'M', period == '1') %>% dplyr::select('CALCIUM', 'sex')
y <- data.study %>% filter(sex == 'M', period == '2') %>% dplyr::select('CALCIUM', 'sex')
z <- data.study %>% filter(sex == 'M') %>% dplyr::select('CALCIUM', 'sex','period')

aggregate(x$CALCIUM ~ x$sex, FUN= "sd" )
aggregate(y$CALCIUM ~ y$sex, FUN= "sd" )
```

```{r}
boxplot(CALCIUM ~ period, data = z, col = "blue", xlab = "Males in period 1", ylab = "Level of Calcium")
```
```{r}
par(mfrow = c(1, 2))
hist(x$CALCIUM, main="Histogram for Males", xlab = "Calcium level of males in period 1")
hist(y$CALCIUM, main="Histogram for Males", xlab = "Calcium level of males in period 2")
```
</br> As we can see the data seems to have a sort of normal distribution and hence a t-test could be justified.
Then we perform the F-test for the t-test like before plus we also do the Wilcox test.
```{r}
var.test(x$CALCIUM,y$CALCIUM, alternative = 'two.sided')
```
As can be seen in the result above the F-test shows us that the variances are different hence we run the t-test with var.equal = FALSE.
```{r}
t.test(x$CALCIUM,y$CALCIUM, conf.level = 1-alpha, var.equal = FALSE)
```
Since the p-value of the Welch Two Sample t-test is very small, we can reject the H0 and say that the means of the control males in period 1 and period 2 is different.
However when we run the Anderson-Darling test to check our assumption of a normal distribution we discover that the p-value is below our 0.05 significance level. And hence we can reject the (H0) assumption that the data is normaly distributed.
```{r}
ad.test(x$CALCIUM)
ad.test(y$CALCIUM)
```

Next we perform the Wilcoxon test again, since it has less ridged assumptions than the t-test we prefer this test.
(The t-test assumes that the distribution of the data is normal, whereas with the wilcoxon we donâ€™t make the assumption that the distribution of the data belongs to a family of parametric distributions)
```{r}
m <- data.study %>% filter(sex == 'M', period == 1) %>% dplyr::select(CALCIUM) %>% na.omit()
f <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(CALCIUM) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
Here we can agian see and confirm that the p value is close to 0 and hence reject the H0 and say that the means of the control males in period 1 and period 2 is different.
We prefer the Wilcoxon test over the t-test because we have shown that the data is not normaly distributed.

**c. test for the males if the mean level of calcium for period 2 is the same for the control group and the MON810 group.**

As per usual we split the dataset into the two groups we want to compare, so here we filter them to only contain males from period 2 and have the control group vs the MON810 group.
Next we perform a F-test to see whether the variances differ between the two groups and use the result for our final t-test.

```{r}
data.study.cont <- data.study %>% filter(sex == 'M', period == '2', regimen == 'control')
data.study.MON810 <- data.study %>% filter(sex == 'M',period == '2', regimen == 'MON810') 
z <- data.study %>% filter(sex == 'M',period == '2', regimen != 'reference1', regimen != 'reference2',regimen != 'reference3',regimen != 'reference4',regimen != 'reference5',regimen != 'reference6') 
```

```{r}
par(mfrow = c(1, 3))
hist(data.study.cont$CALCIUM, main='Histogram of Control Group', xlab = "Calcium level of Males of period 2")
hist(data.study.MON810$CALCIUM, main='Histogram of Test Group',xlab = "Calcium level of Males of period 2")
boxplot(CALCIUM ~ regimen, data = z, col = "blue", xlab = "Calcium level of males of separate groups", ylab="Frequency", main="Boxplot of Calcium levels")
```


```{r}
var.test(data.study.cont$CALCIUM, data.study.MON810$CALCIUM,conf.level = 1-alpha)
t.test(data.study.cont$CALCIUM, data.study.MON810$CALCIUM, conf.level = 1-alpha, var.equal = FALSE)
```
Here we first see that the F-test tells us that the variances are different between the two groups due to the low p-value (0.037) with an alpha of 5%. 
As we can see the p-value of the Welch Two Sample t-test is very high (0.446), so we do not reject the H0 there. This translates into the fact that the means of the males in period 2 of the control and MON810 group do not differ significantly.
However when we run the Anderson-Darling test to check our assumption of a normal distribution we discover that the p-value is around our 0.05 significance level for the control group and below 0.05 for the MON810 group. So one part of the data is normaly distributed and the other is not. However one should not cling on to the significance level of the p-values too much. All we can say here is that they are close to each other and that they are not definitevely normaly distributed but also not not definitevely normaly distributed. In the end, a Wilcoxon test is still the best option to use.
```{r}
ad.test(data.study.cont$CALCIUM)
ad.test(data.study.MON810$CALCIUM)
```

Let's perform a Wilcoxon test as well and see if it concurs with the findings of the t-test
```{r}
m <- data.study %>% filter(sex == 'M', period == 2,regimen == 'control') %>% dplyr::select(CALCIUM) %>% na.omit()
f <- data.study %>% filter(sex == 'M', period == 2,regimen == 'MON810') %>% dplyr::select(CALCIUM) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
Here we get a different p-value than the t-test but both of the p-values signify that we cannot reject the H0 and so the two means of the control and MON810 groups do not differ. As said before, we prefer the Wilcoxon test.

**d. What is the probability to detect a difference of 1 sd (one standard deviation) with only 10 animals per group? with 20 animal? How can we ensure to detect such difference with a probability of 80%?**

So the two groups are comprised of 10 individuals each, so the number of degrees of freedom is 10+10-2=18
Next for the power of the test should increase by the sample size, but the dt stays the same since delta is the standard deviation and we are dividing the delta by the standard deviation so the result will always be 1
```{r}
alpha=0.05
delta <- 1
x.sd <- 1
df <- 18
dt <- delta/x.sd/sqrt(1/10+1/10)
1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df)
```
We can check this by running the code in the function pwr.t.test
Here d=1 because we want to detect 1sd difference and according to Cohen's d we get the formula: |Î¼1-Î¼2|/Ïƒ = 1sd/1sd = 1.
```{r}
pwr.t.test(n=10, d=1, type="two.sample", alternative="two.sided", sig.level=alpha)[[4]]
```
So the probability of detecting 1 sd difference with only 10 animals per group is 55%

Next we try the same but change the sample size to 20 animals per group
```{r}
alpha=0.05
delta <- 1 
x.sd <- 1
df <- 38
dt <- delta/x.sd/sqrt(1/20+1/20)
1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df)
```
We can check this by running the code in the function pwr.t.test.
Here d=1 because we want to detect 1sd difference and according to Cohen's d we get the formula: |Î¼1-Î¼2|/Ïƒ = 1sd/1sd = 1.
```{r}
pwr.t.test(n=20, d=1, type="two.sample", alternative="two.sided", sig.level=alpha)[[4]]
```
So the probability of detecting 1 sd difference with only 20 animals per group is 55%

Next we want to find the number of observations for which the detection of more than 1 sd is equal or higher than 80%.
For this we fill in the power function and set 
Here d=1 because we want to detect 1sd difference and according to Cohen's d we get the formula: |Î¼1-Î¼2|/Ïƒ = 1sd/1sd = 1.
```{r}
pwr.t.test(power=0.8, d=1, sig.level=alpha) 
```
So we have to set n=17 for the probability to be able to detect a difference of 1 standard deviation to be higher than 80%.
The only problem with running all these pwr.t.test functions is that it assumes that the data are normaly distributed, which is something that we have disproven in previous questions.

**e. Test for the males if the mean levels of calcium for period 2 of the control group and the MON810 are equivalent. The equivalence limits will be defined using the 6 reference groups as i) one standard deviation of the 6 reference means, ii) two standard deviations of the 6 reference means.**
     
First we filter the data
```{r}
data.study.cont <- data.study %>% filter(sex == 'M', period == '2', regimen == 'control')
data.study.MON810 <- data.study %>% filter(sex == 'M',period == '2', regimen == 'MON810') 
data.study.ref <- data.study %>% filter(sex == 'M',period == '2', regimen != 'control', regimen != 'MON810') %>%  dplyr::select(regimen, CALCIUM) %>% na.omit
```

     *i)* one standard deviation of the 6 reference means. 
Next we calculate the standard deviation of the 6 reference means
```{r}
ref <- aggregate(data.study.ref$CALCIUM ~ data.study.ref$regimen, FUN = 'mean')
sd(ref$`data.study.ref$CALCIUM`)
```
This value we use in the pt function.
We calculate the degrees of freedom by taking the number of observations and subtracting the number of classes/categories: 58-6=52

```{r}
delta <- sd(ref$`data.study.ref$CALCIUM`)
tost(data.study.MON810$CALCIUM, data.study.cont$CALCIUM, alpha=0.05, epsilon=delta)
```
So the null hypothesis is not rejected, hence the male means of the calcium levels in period two are not within the equivalence level range of 1sd of the reference means.

     
     *ii)* two standard deviations of the 6 reference means.
Now the only difference is that we multiply the delta by 2 since we want to take two sd.
```{r}
delta <- 2* sd(ref$`data.study.ref$CALCIUM`)
tost(data.study.MON810$CALCIUM, data.study.cont$CALCIUM, alpha=0.05, epsilon=delta)
```
So the null hypothesis is not rejected, hence the male means of the calcium levels in period two are not within the equivalence level range of 2 sd of the reference means.

The only problem with running all these tost test functions is that it assumes that the data are normaly distributed, which is something that we have disproven in previous questions.
    

**f. Summarize and comment these results.** 
</br> So in the end all the data we analyzed was not normaly distributed and hence the way to go was to use the Wilcoxon test. With that said we showed that:
  -the means of calcium levels for males and females in period 2 are different (Wilcoxon test)
  -the means of calcium of the control males in period 1 and period 2 are different (Wilcoxon test)
  -the two calcium means of the control and MON810 groups do not differ (Wilcoxon test)
  -the probability of detecting 1 sd difference with only 10 animals per group is 55% (power test)
  -we should set n=17 for the probability to be able to detect a difference of 1 standard deviation to be higher than 80%. (power test)
  -the male means of the calcium levels in period two are not within the equivalence level range of 1sd of the reference means. (tost test)
  -the male means of the calcium levels in period two are not within the equivalence level range of 2 sd of the reference means. (tost test)
  
I believe I have commented enough during the questions themselves but just to be sure I have written a little piece of comentary on these tests and procedures in general.
My comments on these results are that one should that first always look at the distribution of the data. If we are checking if two means are similar and the data is normaly distributed we should use a t-test. However, if the data is not normaly distributed it is better to use Mann-Whitney-Wilcoxon test. On a side note, when one is using the t-test one should also check for equivalence of variance by performing the F-test. 
When you want to detect a certain difference between two groups we should use the power test which allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. The power of our test is 56% which is basically the ability to reject H0 when H0 is truly false with a difference of one standard deviation. We could increase this percentage by either increasing the sample size or by lowering the power level.

In the end a lot of these tests would have different outcomes if we were to increase the sample size. 

  
  
# Do the same analysis with the variable "DIRBILI" (direct bilirubin)


**a)test if the mean level of calcium for period 2 is the same for males and females.**
Hint: plot first the data and justify the test(s) to use.

   Here we unfortunately cannot do the same test as before since DIRBILI is discreet data in period 2.
   Nevertheless we plot the data and see what we can learn from it.

```{r}
dirbili <- data.study %>% dplyr::select(regimen,sex,period,DIRBILI) %>% na.omit()
dirbili$period <- as.factor(dirbili$period)
```

```{r}
#Splitting the dataset into the two periods 
dirbili.p1 <- dirbili %>% filter(period == 1)
dirbili.p2 <- dirbili %>% filter(period == 2)

#Splitting the dataset into period 1 and 2 and into male and female and plotting the result
dirbili.p2.m <- dirbili.p2 %>% filter(sex == 'M')
dirbili.p2.f <- dirbili.p2 %>% filter(sex == 'F')

dirbili.p1.m <- dirbili.p1 %>% filter(sex == 'M')
dirbili.p1.f <- dirbili.p1 %>% filter(sex == 'F')

new <- aggregate(dirbili$DIRBILI ~ dirbili$sex, FUN= "mean") 
colnames(new) <- c('sex', 'dirbili')


ggplot() +
  geom_point(data = dirbili.p2.m, aes(x=sex, y=DIRBILI, colour = 'blue'), show.legend = F) + 
  geom_point(data = dirbili.p2.f, aes(x=sex, y=DIRBILI, colour = 'red'), show.legend = F) +
  geom_point(data = new, aes(x=sex, y=dirbili), size=5, alpha=0.5) + 
  xlab("Sex") + 
  ylab("Dirbili level")
par(mfrow = c(1, 2))
hist(dirbili.p2.f$DIRBILI, main='Histogram of DIRBILI for females', xlab = "Dirbili levels")
hist(dirbili.p2.m$DIRBILI, main='Histogram of DIRBILI for male', xlab = "Dirbili levels")

```

```{r}
table(dirbili.p2 %>% dplyr::select(sex,DIRBILI))
```
Since this looks like categorical data and hence split into different classes we perform a chi squared test on the continuity table.
```{r}
chisq.test(table(dirbili.p2 %>% dplyr::select(sex,DIRBILI)), correct = FALSE)
```
As we can see the p value is large and hence the H0 holds where we say that the means of the two groups male and female in period two do not differ.

But if we assume that this is not categorical data we still use our same old wilcoxon test.
```{r}
m <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(DIRBILI) %>% na.omit()
f <- data.study %>% filter(sex == 'F', period == 2) %>% dplyr::select(DIRBILI) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
The wilcoxon test does not reject the H0 and comes to the same conclusion as the chi-squared test. The H0 in this case is that there is no difference in the DIRBILI means of males and females in period 2. As before we actually prefer the wilcoxon test to the chi-squared test due to the fact that we are not dealing with according to the paper. Furthermore this is also more in line with the questions of exercise 3.

** b) Test for the males if the mean level of dirbili is the same for period 1 and period 2.**
```{r}
new <- aggregate(dirbili$DIRBILI ~ dirbili$period, FUN= "mean") 
colnames(new) <- c('period', 'dirbili')

ggplot() +
  geom_point(data = dirbili.p1.m, aes(x=period, y=DIRBILI, colour = period), show.legend = F) + 
  geom_point(data = dirbili.p2.m, aes(x=period, y=DIRBILI, colour = period), show.legend = F) +
  geom_point(data = new, aes(x=period, y=dirbili), size=5, alpha=0.5) +
  ylab("Dirbili levels") + 
  xlab("Period")

par(mfrow = c(1, 2))
hist(dirbili.p1.m$DIRBILI, main='Histogram of DIRBILI for males P=1', xlab = "Dirbili Levels")
hist(dirbili.p2.m$DIRBILI, main='Histogram of DIRBILI for males P=2', xlab = "Dirbili Levels")
```


```{r}
table(dirbili %>% dplyr::select(period,DIRBILI))
```
Here we use the chi squared test again with the assumption that the data is categorical.
Since correction is only available for 2 by 2 tables we do not need to write it in the chi squared formula.
```{r}
chisq.test(table(dirbili %>% dplyr::select(period,DIRBILI)))
```
So we can reject the H0 hence we can say that the two means are different.

But if we assume that this is not categorical data we still use the wilcoxon test.
```{r}
m <- data.study %>% filter(sex == 'M', period == 1) %>% dplyr::select(DIRBILI) %>% na.omit()
f <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(DIRBILI) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
The wilcoxon test also rejects the H0 and comes to the same conclusion as the chi-squared test. The H0 is that there is a difference between the male DIRBILI means of period 1 and 2. As before we actually prefer the wilcoxon test to the chi-squared test. This is also more in line with the questions of exercise 3.


**c)test for the males if the mean level of Dirbili for period 2 is the same for the control group and the MON810 group.**
```{r}
dirbili.cont <- dirbili %>% filter(sex == 'M', period == '2', regimen == 'control')
dirbili.MON810 <- dirbili %>% filter(sex == 'M',period == '2', regimen == 'MON810') 
```

```{r}
new <- aggregate(dirbili.p2.m$DIRBILI ~ dirbili.p2.m$regimen, FUN= "mean")
colnames(new) <- c('regimen', 'dirbili')
new <- new %>% filter(regimen %in% c('control','MON810')) 

unique(dirbili.p2.m)

ggplot() +
  geom_point(data = dirbili.p2.m %>% filter(regimen == 'control'), aes(x=regimen, y=DIRBILI, colour = regimen), show.legend = F) + 
  geom_point(data = dirbili.p2.m %>% filter(regimen == 'MON810'), aes(x=regimen, y=DIRBILI, colour = regimen), show.legend = F) +
  geom_point(data = new, aes(x=regimen, y=dirbili), size=5, alpha=0.5) + 
  ylab("Dirbili level") +
  xlab("Regimen")


par(mfrow = c(1, 2))
hist(dirbili.cont$DIRBILI, main='Histogram of DIRBILI for male control', xlab = "Dirbili level")
hist(dirbili.MON810$DIRBILI, main='Histogram of DIRBILI for male MON810', xlab = "Dirbili level")
```
As we can see here the two means are the same so we perform another chisquared test under the assumption of it being categorical.
```{r}
table(dirbili.p2.m  %>% filter(regimen %in% c('control','MON810')) %>% dplyr::select(regimen, DIRBILI))[1:2,]
```

```{r}
chisq.test(table(dirbili.p2.m  %>% filter(regimen %in% c('control','MON810')) %>% dplyr::select(regimen, DIRBILI))[1:2,])
```
Here the p-value is equal to 1 which makes sense since the means are the same and we have the same number of observations per dirbili group. 
But if we assume that this is not categorical data we use the wilcoxon test.
```{r}
m <- data.study %>% filter(sex == 'M', period == 2,regimen == 'control') %>% dplyr::select(DIRBILI) %>% na.omit()
f <- data.study %>% filter(sex == 'M', period == 2,regimen == 'MON810') %>% dplyr::select(DIRBILI) %>% na.omit()
x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
x
```
The wilcoxon test also does not rejects the H0 and comes to the same conclusion as the chi-squared test. As before we actually prefer the wilcoxon test to the chi-squared test. This is also more in line with the questions of exercise 3.

**d) What is the probability to detect a difference of 1 sd (one standard deviation) with only 10 animals per group? with 20 animal? How can we ensure to detect such difference with a probability of 80%?**

Here we compare two proportions we use the pwr.2p.test where n=10 individuals per class and h=0.5 
```{r}
pwr.2p.test(n=10, h=1, alternative = 'two.sided', sig.level = 0.05)
```
So the probability to detect a difference of 1sd with only 10 animals per group is 61%

The same formula is used for the second part but we adjust n =20 for the larger sample group.
Here d=1 because we want to detect 1 sd difference and according to Cohen h should roughly be equal to 0.2 0.5 or 0.8 for small medium and large effect sizes respectively. Since our data is going to be randomly chosen we cannot calculate the h exactly. So my guess is that a difference of 1 sd corresponds to a medium effect size of 0.5.
```{r}
pwr.2p.test(n=20, h=0.5, alternative = 'two.sided', sig.level = 0.05)
```
So the probability to detect a difference of 1 sd with only 10 animals per group is 35%.

Next we want to find the number of observations for which the detection of more than 1 sd is equal or higher than 80%.
For this we fill in the power function and set h equal to 1 because we want to detect 1sd difference and according to Cohen's d we get the formula: |Î¼1-Î¼2|/Ïƒ = 1sd/1sd = 1.
```{r}
pwr.2p.test(power=0.8, h=0.5, sig.level=alpha) 
```
So we have to set n=63 for the probability to be able to detect a difference of 1 standard deviation to be higher than 80%.


**e) Test for the males if the mean levels of dirbili for period 2 of the control group and the MON810 are equivalent. The equivalence limits will be defined using the 6 reference groups as i) one standard deviation of the 6 reference means, ii) two standard deviations of the 6 reference means.**

First we filter the data
```{r}
data.study.cont <- data.study %>% filter(sex == 'M', period == '2', regimen == 'control')
data.study.MON810 <- data.study %>% filter(sex == 'M',period == '2', regimen == 'MON810') 
data.study.ref <- data.study %>% filter(sex == 'M',period == '2', regimen != 'control', regimen != 'MON810') %>%  dplyr::select(regimen, DIRBILI) %>% na.omit
```

     *i)* one standard deviation of the 6 reference means. 
Next we calculate the standard deviation of the 6 reference means
```{r}
ref <- aggregate(data.study.ref$DIRBILI ~ data.study.ref$regimen, FUN = 'mean')
sd(ref$`data.study.ref$DIRBILI`)
```

```{r}
delta <- sd(ref$`data.study.ref$DIRBILI`)
tost(data.study.MON810$DIRBILI, data.study.cont$DIRBILI, alpha=0.05, epsilon=delta)
```
So the null hypothesis is not rejected, hence the male means of the DIRBILI levels in period two are not the same when we take the equivalence limit of 1 sd of the reference means.

     
     *ii)* two standard deviations of the 6 reference means.
```{r}
delta <- 2* sd(ref$`data.study.ref$DIRBILI`)
tost(data.study.MON810$DIRBILI, data.study.cont$DIRBILI, alpha=0.05, epsilon=delta)
```
Now the only difference is that we multiply the delta by 2 since we want to take two sd.
Here we can say that the p-value is around our significance level so we cannot actually reject the H0 saying that the control and MON810 are not equivalent within 2 standard deviations.

**f. Summarize and comment these results.**
So in the end all the data we analyzed was not normaly distributed and hence the way to go was to use the Wilcoxon test. With that said we showed that:
  -the means of DIRBILI levels for males and females in period 2 are not different (Wilcoxon test)
  -the means of DIRBILI of the control males in period 1 and period 2 is different (Wilcoxon test)
  -the two DIRBILI means of the control and MON810 groups do not differ (p-value of 1?!) (Wilcoxon test)
  -the probability of detecting 1 sd difference (of DIRBILI) with only 10 animals per group is 35% (power test)
  -we should set n=63 for the probability to be able to detect a difference of 1 standard deviation to be higher than 80%. (power test)
  -the male means of the DIRBILI levels in period two are not within the equivalence level range of 1sd of the reference means. (tost test)
  -the male means of the DIRBILI levels in period two are not within the equivalence level range of 2 sd of the reference means. (tost test)
  
I believe I have commented enough during the questions themselves but just to be sure I have written a little piece of comentary on these tests and procedures in general.
My comments on these results are that one should that first always look at the distribution of the data. If we are checking if two means are similar and the data is normaly distributed we should use a t-test. However, if the data is not normaly distributed it is better to use Mann-Whitney-Wilcoxon test. On a side note, when one is using the t-test one should also check for equivalence of variance by performing the F-test. This data however seems to be categorical if you just look at the values, DIRBILI is either 0.001, 0.01 or 0.1. To handle categorical data one can perform the chi-squared test. However in our case we assume the data is not categorical since we follow the ANSES report. Thus we stick with the Wilcoxon test.
When you want to detect a certain difference between two groups we should use the power test which allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. The power of our test is 89% which is basically the ability to reject H0 when H0 is truly false with a difference of one standard deviation. We could increase this percentage by either increasing the sample size or by lowering the power level. Here we used t 


</br>


# Multiple comparisons
**a. Redo the three tests of the previous section (questions a., b. and c.) for now comparing  the means of all the quantitative variables (see the annex 5 of the ANSES report <ttt>BIOT2009sa0285Ra.pdf</ttt> to know the type of each variable). Store the results (i.e. all the p-values) in a dataframe with one variable per row and four columns (name of the variable + three p-values). **

First we make the functions per question using the Wilcoxon test
```{r}
question.a_wilcox <- function(variable){
  m <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(variable) %>% na.omit()
  f <- data.study %>% filter(sex == 'F', period == 2) %>% dplyr::select(variable) %>% na.omit()
  if (dim(m)[1] > 0 && dim(f)[1]>0){
    x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
    return(x$p.value)
  }
  else{
    return(NA)
  }
}
question.a_wilcox('CALCIUM')
```

```{r}
question.b_wilcox <- function(variable){
  m <- data.study %>% filter(sex == 'M', period == 1) %>% dplyr::select(variable) %>% na.omit()
  f <- data.study %>% filter(sex == 'M', period == 2) %>% dplyr::select(variable) %>% na.omit()
  if(dim(m)[1] > 0 && dim(f)[1] > 0){
    x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
    return(x$p.value)
  }
  else{
    return(NA)
  }
}

question.b_wilcox('PTSEC')
```

```{r}
question.c_wilcox <- function(variable){
  m <- data.study %>% filter(sex == 'M', period == 2,regimen == 'control') %>% dplyr::select(variable) %>% na.omit()
  f <- data.study %>% filter(sex == 'M', period == 2,regimen == 'MON810') %>% dplyr::select(variable) %>% na.omit()
  if(dim(m)[1] > 0 && dim(f)[1] > 0){  
    x <- wilcox.test(m[,1], f[,1], alternative = 'two.sided', conf.level=1-0.05)
    return(x$p.value)
  }
  else{
    return(NA)
  }  
}
question.c_wilcox('OVARY')
```

Now we make all the functions for the same questions using the t-test
```{r}
question.a_ttest <- function(variable){
  alpha<-0.05
  m <- data.study %>% filter(period == 2, sex=='M') %>% dplyr::select(sex, variable) %>% na.omit
  f <- data.study %>% filter(period == 2, sex=='F') %>% dplyr::select(sex, variable) %>% na.omit
  if(dim(m)[1] > 0 && dim(f)[1] > 0){
    fval <- var.test(m[,2],f[,2], alternative = 'two.sided')$p.val >0.05
    y <- t.test(m[,2],f[,2], conf.level=1-alpha, var.equal = fval)
    return(y$p.value)
  }
  else{
    return(NA)
  } 
}
question.a_ttest('CALCIUM')
```

```{r}
question.b_ttest <- function(variable){
  x <- data.study %>% filter(sex == 'M', period == '1') %>% dplyr::select(sex, variable) %>% na.omit()
  y <- data.study %>% filter(sex == 'M', period == '2') %>% dplyr::select(sex, variable) %>% na.omit()
  if(dim(x)[1] > 0 && dim(y)[1] > 0){
    fval<-var.test(x[,2],y[,2], alternative = 'two.sided')$p.val > 0.05
    m <- t.test(x[,2],y[,2], conf.level = 1-0.05, var.equal = fval)
    return(m$p.value)
  }
  else{
    return(NA)
  }
}
question.b_ttest('PTSEC')
```

```{r}
question.c_ttest <- function(variable){
  c <- data.study %>% filter(sex == 'M', period == '2', regimen == 'control') %>% dplyr::select(regimen, variable) %>% na.omit()
  m <- data.study %>% filter(sex == 'M',period == '2', regimen == 'MON810') %>% dplyr::select(regimen, variable)%>% na.omit()
  if(dim(c)[1] > 0 && dim(m)[1] > 0){
    fval <- var.test(c[,2], m[,2],conf.level = 1-alpha)$p.val >0.05
    r <- t.test(c[,2], m[,2], conf.level = 1-alpha, var.equal = FALSE)
    return(r$p.value)
  }
  else{
    return(NA)
  }
}
question.c_ttest('CALCIUM')
```

Next we apply these functions to all the other quantitative variables in the table and save the results in a dataframe.
First we have to make the list of all the column names we want to loop over and use as input for our functions.
```{r}
variable <- data.study  %>% dplyr::select(-GLUCOSE, -BLOOD, -BILI, -PROT ,-KETONE, -UROBILI, -GAMMAGT,-id,-X,-regimen,-sex,-period) %>% colnames()
```


```{r, warning=FALSE}
wilcox <- data.frame()
ttest <- data.frame()
for (i in 1:length(variable)){
  # Wilcox results
  wilcox <- rbind(wilcox, cbind(variable[i], question.a_wilcox(variable[i]), question.b_wilcox(variable[i]), question.c_wilcox(variable[i])))
  # t-test results
  ttest <- rbind(ttest, cbind(variable[i], question.a_ttest(variable[i]), question.b_ttest(variable[i]), question.c_ttest(variable[i])))
}

colnames(wilcox) <- c('variable', 'a', 'b', 'c')
colnames(ttest) <- c('variable', 'a', 'b', 'c')
```
We should actually only look at the wilcox dataframe since we have seen in the previous questions that the data so far has not been distributed normaly. So the best test would then be the wilcoxon test as explained previously.


**b. For each of the three tests, adjust the p-values using the Bonferroni  and the Benjamini-Hochberg corrections. How can we interpret these results?**
   
First we convert the variable column into characters and the pvalue into numeric in order to perform smooth opperations on them.
```{r}
wilcox$variable <- as.character(wilcox$variable)
wilcox$a <- as.numeric(paste(wilcox$a))
wilcox$b <- as.numeric(paste(wilcox$b))
wilcox$c <- as.numeric(paste(wilcox$c))

ttest$variable <- as.character(ttest$variable)
ttest$a <- as.numeric(paste(ttest$a))
ttest$b <- as.numeric(paste(ttest$b))
ttest$c <- as.numeric(paste(ttest$c))
```
Next we apply the p.adjust function to the different variables in the first column and prin the result in the corresponding place. The p.adjust function lets us choose the method of p-value adjustment. Due to the length of the notebook so far I decided to not write all the separate code and do it all by hand if there is a function. The result is below.
```{r}
wilcoxBonferroni <- as.data.frame(variable)
wilcoxBonferroni$`a Bonferroni` <- p.adjust(wilcox$a, method = "bonferroni", n = 60-sum(is.na(wilcox$a)))
wilcoxBonferroni$`b Bonferroni` <- p.adjust(wilcox$b, method = "bonferroni", n = 60-sum(is.na(wilcox$b)))
wilcoxBonferroni$`c Bonferroni` <- p.adjust(wilcox$c, method = "bonferroni", n = 60-sum(is.na(wilcox$c)))

ttestBonferroni <- as.data.frame(variable)
ttestBonferroni$`a Bonferroni` <- p.adjust(ttest$a, method = "bonferroni", n = 60-sum(is.na(ttest$a)))
ttestBonferroni$`b Bonferroni` <- p.adjust(ttest$b, method = "bonferroni", n = 60-sum(is.na(ttest$b)))
ttestBonferroni$`c Bonferroni` <- p.adjust(ttest$c, method = "bonferroni", n = 60-sum(is.na(ttest$c)))
```

```{r}
wilcoxBH <- as.data.frame(variable)
wilcoxBH$`a BH` <- p.adjust(wilcox$a, method = "BH", n = 60-sum(is.na(wilcox$a)))
wilcoxBH$`b BH` <- p.adjust(wilcox$b, method = "BH", n = 60-sum(is.na(wilcox$b)))
wilcoxBH$`c BH` <- p.adjust(wilcox$c, method = "BH", n = 60-sum(is.na(wilcox$c)))

ttestBH <- as.data.frame(variable)
ttestBH$`a BH` <- p.adjust(ttest$a, method = "BH", n = 60-sum(is.na(ttest$a)))
ttestBH$`b BH` <- p.adjust(ttest$b, method = "BH", n = 60-sum(is.na(ttest$b)))
ttestBH$`c BH` <- p.adjust(ttest$c, method = "BH", n = 60-sum(is.na(ttest$c)))
```

However to show that I understand the concept of Bonferroni correction we have to just multiply the original p-value obtained by the wilcoxon test by the number of variables in the column, in our case the first column contains 58 variables (there are 2 NA's) the second and third column contain 49 and 59 respectively.
The Benjamini-Hochberg corrections can be obtained by putting the p-values in ascending order, then rank the values (from 1 to 60). Next we compute the Benjamini-Hochberg critical value by: (i/m)*Q, where:
i = rank,
m = total number of tests (60 in our case),
Q = the false discovery rate (a percentage, default is 0.05).
The obtained value is then compared to the original p-value, we find the largest p-value which is smaller than the critical value. This variable and all the ones above it are then considered significant by the Benjamini-Hochberg corretion and hence reject the H0.

We can interpret the Bonferroni as a correction which will decrease the number of false positives (type 1 error). Basically if you run a lot of hypotheses there is a higher chance of getting a small p-value by chance.
We can interpret the Benjamini-Hochberg as a correction that will decrease the number of false positives (type 1 error) just like the Bonferroni correction. It basically adjusts for the fact that sometimes there are going to be small p-values by chance.

